{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_util.py\n",
    "\n",
    "import os\n",
    "import ujson\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 10\n",
    "train_ratio = 0.75 # merge original training set and test set, then split it manually. \n",
    "alpha = 0.1 # for Dirichlet distribution\n",
    "\n",
    "def check(config_path, train_path, test_path, num_clients, niid=False, \n",
    "        balance=True, partition=None):\n",
    "    # check existing dataset\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = ujson.load(f)\n",
    "        if config['num_clients'] == num_clients and \\\n",
    "            config['non_iid'] == niid and \\\n",
    "            config['balance'] == balance and \\\n",
    "            config['partition'] == partition and \\\n",
    "            config['alpha'] == alpha and \\\n",
    "            config['batch_size'] == batch_size:\n",
    "            print(\"\\nDataset already generated.\\n\")\n",
    "            return True\n",
    "\n",
    "    dir_path = os.path.dirname(train_path)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    dir_path = os.path.dirname(test_path)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    return False\n",
    "\n",
    "def separate_data(data, num_clients, num_classes, niid=False, balance=False, partition=None, class_per_client=None):\n",
    "    X = [[] for _ in range(num_clients)]\n",
    "    y = [[] for _ in range(num_clients)]\n",
    "    statistic = [[] for _ in range(num_clients)]\n",
    "\n",
    "    dataset_content, dataset_label = data\n",
    "    # guarantee that each client must have at least one batch of data for testing. \n",
    "    least_samples = int(min(batch_size / (1-train_ratio), len(dataset_label) / num_clients / 2))\n",
    "\n",
    "    dataidx_map = {}\n",
    "\n",
    "    if not niid:\n",
    "        partition = 'pat'\n",
    "        class_per_client = num_classes\n",
    "\n",
    "    if partition == 'pat':\n",
    "        idxs = np.array(range(len(dataset_label)))\n",
    "        idx_for_each_class = []\n",
    "        for i in range(num_classes):\n",
    "            idx_for_each_class.append(idxs[dataset_label == i])\n",
    "       \n",
    "        class_num_per_client = [class_per_client for _ in range(num_clients)]\n",
    "        for i in range(num_classes):\n",
    "            selected_clients = []\n",
    "            for client in range(num_clients):\n",
    "                if class_num_per_client[client] > 0:\n",
    "                    selected_clients.append(client)\n",
    "            selected_clients = selected_clients[:int(np.ceil((num_clients/num_classes)*class_per_client))]\n",
    "\n",
    "            num_all_samples = len(idx_for_each_class[i])\n",
    "            num_selected_clients = len(selected_clients)\n",
    "            num_per = num_all_samples / num_selected_clients\n",
    "            if balance:\n",
    "                num_samples = [int(num_per) for _ in range(num_selected_clients-1)]\n",
    "            else:\n",
    "                num_samples = np.random.randint(max(num_per/10, least_samples/num_classes), num_per, num_selected_clients-1).tolist()\n",
    "            num_samples.append(num_all_samples-sum(num_samples))\n",
    "\n",
    "            idx = 0\n",
    "            for client, num_sample in zip(selected_clients, num_samples):\n",
    "                if client not in dataidx_map.keys():\n",
    "                    dataidx_map[client] = idx_for_each_class[i][idx:idx+num_sample]\n",
    "                else:\n",
    "                    dataidx_map[client] = np.append(dataidx_map[client], idx_for_each_class[i][idx:idx+num_sample], axis=0)\n",
    "                idx += num_sample\n",
    "                class_num_per_client[client] -= 1\n",
    "\n",
    "    elif partition == \"dir\":\n",
    "        # https://github.com/IBM/probabilistic-federated-neural-matching/blob/master/experiment.py\n",
    "        min_size = 0\n",
    "        K = num_classes\n",
    "        N = len(dataset_label)\n",
    "\n",
    "        try_cnt = 1\n",
    "        while min_size < least_samples:\n",
    "            if try_cnt > 1:\n",
    "                print(f'Client data size does not meet the minimum requirement {least_samples}. Try allocating again for the {try_cnt}-th time.')\n",
    "\n",
    "            idx_batch = [[] for _ in range(num_clients)]\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(dataset_label == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "                proportions = np.array([p*(len(idx_j)<N/num_clients) for p,idx_j in zip(proportions,idx_batch)])\n",
    "                proportions = proportions/proportions.sum()\n",
    "                proportions = (np.cumsum(proportions)*len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_batch,np.split(idx_k,proportions))]\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "            try_cnt += 1\n",
    "\n",
    "        for j in range(num_clients):\n",
    "            dataidx_map[j] = idx_batch[j]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # assign data\n",
    "    for client in range(num_clients):\n",
    "        idxs = dataidx_map[client]\n",
    "        X[client] = dataset_content[idxs]\n",
    "        y[client] = dataset_label[idxs]\n",
    "\n",
    "        for i in np.unique(y[client]):\n",
    "            statistic[client].append((int(i), int(sum(y[client]==i))))\n",
    "            \n",
    "\n",
    "    del data\n",
    "    # gc.collect()\n",
    "\n",
    "    for client in range(num_clients):\n",
    "        print(f\"Client {client}\\t Size of data: {len(X[client])}\\t Labels: \", np.unique(y[client]))\n",
    "        print(f\"\\t\\t Samples of labels: \", [i for i in statistic[client]])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return X, y, statistic\n",
    "\n",
    "  \n",
    "def split_data(X, y):\n",
    "    # Split dataset\n",
    "    train_data, test_data = [], []\n",
    "    num_samples = {'train':[], 'test':[]}\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X[i], y[i], train_size=train_ratio, shuffle=True)\n",
    "\n",
    "        train_data.append({'x': X_train, 'y': y_train})\n",
    "        num_samples['train'].append(len(y_train))\n",
    "        test_data.append({'x': X_test, 'y': y_test})\n",
    "        num_samples['test'].append(len(y_test))\n",
    "\n",
    "    print(\"Total number of samples:\", sum(num_samples['train'] + num_samples['test']))\n",
    "    print(\"The number of train samples:\", num_samples['train'])\n",
    "    print(\"The number of test samples:\", num_samples['test'])\n",
    "    print()\n",
    "    del X, y\n",
    "    # gc.collect()\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def save_file(config_path, train_path, test_path, train_data, test_data, num_clients, \n",
    "                num_classes, statistic, niid=False, balance=True, partition=None):\n",
    "    config = {\n",
    "        'num_clients': num_clients, \n",
    "        'num_classes': num_classes, \n",
    "        'non_iid': niid, \n",
    "        'balance': balance, \n",
    "        'partition': partition, \n",
    "        'Size of samples for labels in clients': statistic, \n",
    "        'alpha': alpha, \n",
    "        'batch_size': batch_size, \n",
    "    }\n",
    "\n",
    "    # gc.collect()\n",
    "    print(\"Saving to disk.\\n\")\n",
    "\n",
    "    for idx, train_dict in enumerate(train_data):\n",
    "        with open(train_path + str(idx) + '.npz', 'wb') as f:\n",
    "            np.savez_compressed(f, data=train_dict)\n",
    "    for idx, test_dict in enumerate(test_data):\n",
    "        with open(test_path + str(idx) + '.npz', 'wb') as f:\n",
    "            np.savez_compressed(f, data=test_dict)\n",
    "    with open(config_path, 'w') as f:\n",
    "        ujson.dump(config, f)\n",
    "\n",
    "    print(\"Finish generating dataset.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit5\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from os import path\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def dense_to_one_hot(labels_dense):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    labels_one_hot = np.zeros((len(labels_dense),))\n",
    "    labels_dense = list(labels_dense)\n",
    "    for i, t in enumerate(labels_dense):\n",
    "        if t == 10:\n",
    "            t = 0\n",
    "            labels_one_hot[i] = t\n",
    "        else:\n",
    "            labels_one_hot[i] = t\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def load_mnist(base_path):\n",
    "    print(\"load mnist\")\n",
    "    mnist_data = loadmat(path.join(base_path, \"mnist_data.mat\"))\n",
    "    mnist_train = np.reshape(mnist_data['train_32'], (55000, 32, 32, 1))\n",
    "    mnist_test = np.reshape(mnist_data['test_32'], (10000, 32, 32, 1))\n",
    "    # turn to the 3 channel image with C*H*W\n",
    "    mnist_train = np.concatenate([mnist_train, mnist_train, mnist_train], 3)\n",
    "    mnist_test = np.concatenate([mnist_test, mnist_test, mnist_test], 3)\n",
    "    mnist_train = mnist_train.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "    mnist_test = mnist_test.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "    # get labels\n",
    "    mnist_labels_train = mnist_data['label_train']\n",
    "    mnist_labels_test = mnist_data['label_test']\n",
    "    # random sample 25000 from train dataset and random sample 9000 from test dataset\n",
    "    train_label = np.argmax(mnist_labels_train, axis=1)\n",
    "    inds = np.random.permutation(mnist_train.shape[0])\n",
    "    mnist_train = mnist_train[inds]\n",
    "    train_label = train_label[inds]\n",
    "    test_label = np.argmax(mnist_labels_test, axis=1)\n",
    "\n",
    "    mnist_train = mnist_train[:25000]\n",
    "    train_label = train_label[:25000]\n",
    "    mnist_test = mnist_test[:9000]\n",
    "    test_label = test_label[:9000]\n",
    "    #print('mnist',mnist_train.shape,train_label.shape, mnist_test.shape,test_label.shape)\n",
    "    return mnist_train, train_label, mnist_test, test_label\n",
    "\n",
    "\n",
    "def load_mnist_m(base_path):\n",
    "    print(\"load mnist_m\")\n",
    "    mnistm_data = loadmat(path.join(base_path, \"mnistm_with_label.mat\"))\n",
    "    mnistm_train = mnistm_data['train']\n",
    "    mnistm_test = mnistm_data['test']\n",
    "    mnistm_train = mnistm_train.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "    mnistm_test = mnistm_test.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "    # get labels\n",
    "    mnistm_labels_train = mnistm_data['label_train']\n",
    "    mnistm_labels_test = mnistm_data['label_test']\n",
    "    # random sample 25000 from train dataset and random sample 9000 from test dataset\n",
    "    train_label = np.argmax(mnistm_labels_train, axis=1)\n",
    "    inds = np.random.permutation(mnistm_train.shape[0])\n",
    "    mnistm_train = mnistm_train[inds]\n",
    "    train_label = train_label[inds]\n",
    "    test_label = np.argmax(mnistm_labels_test, axis=1)\n",
    "    mnistm_train = mnistm_train[:25000]\n",
    "    train_label = train_label[:25000]\n",
    "    mnistm_test = mnistm_test[:9000]\n",
    "    test_label = test_label[:9000]\n",
    "   # print('mnistm',mnistm_train.shape,train_label.shape, mnistm_test.shape,test_label.shape)\n",
    "    return mnistm_train, train_label, mnistm_test, test_label\n",
    "\n",
    "\n",
    "def load_svhn(base_path):\n",
    "    print(\"load svhn\")\n",
    "    svhn_train_data = loadmat(path.join(base_path, \"svhn_train_32x32.mat\"))\n",
    "    svhn_test_data = loadmat(path.join(base_path, \"svhn_test_32x32.mat\"))\n",
    "    svhn_train = svhn_train_data['X']\n",
    "    svhn_train = svhn_train.transpose(3, 2, 0, 1).astype(np.float32)\n",
    "    svhn_test = svhn_test_data['X']\n",
    "    svhn_test = svhn_test.transpose(3, 2, 0, 1).astype(np.float32)\n",
    "    train_label = svhn_train_data[\"y\"].reshape(-1)\n",
    "    test_label = svhn_test_data[\"y\"].reshape(-1)\n",
    "    inds = np.random.permutation(svhn_train.shape[0])\n",
    "    svhn_train = svhn_train[inds]\n",
    "    train_label = train_label[inds]\n",
    "    svhn_train = svhn_train[:25000]\n",
    "    train_label = train_label[:25000]\n",
    "    svhn_test = svhn_test[:9000]\n",
    "    test_label = test_label[:9000]\n",
    "    train_label[train_label == 10] = 0\n",
    "    test_label[test_label == 10] = 0\n",
    "    #print('svhn',svhn_train.shape,train_label.shape, svhn_test.shape,test_label.shape)\n",
    "    return svhn_train, train_label, svhn_test, test_label\n",
    "\n",
    "\n",
    "def load_syn(base_path):\n",
    "    print(\"load syn\")\n",
    "\n",
    "    syn_data = loadmat(path.join(base_path, \"syn_number.mat\"))\n",
    "    syn_train = syn_data['train_data']\n",
    "    syn_test =  syn_data['test_data']\n",
    "    syn_train = syn_train.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "    syn_test = syn_test.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "    syn_labels_train = syn_data['train_label']\n",
    "    syn_labels_test = syn_data['test_label']\n",
    "\n",
    "    train_label = syn_labels_train\n",
    "    inds = np.random.permutation(syn_train.shape[0])\n",
    "    syn_train = syn_train[inds]\n",
    "    train_label = train_label[inds]\n",
    "    test_label = syn_labels_test\n",
    "\n",
    "    # syn_train = syn_train[:25000]\n",
    "    # train_label = train_label[:25000]\n",
    "    # syn_test = syn_test[:9000]\n",
    "    # test_label = test_label[:9000]\n",
    "    train_label = dense_to_one_hot(train_label)\n",
    "    test_label = dense_to_one_hot(test_label)\n",
    "    #print('syn',syn_train.shape,train_label.shape, syn_test.shape,test_label.shape)\n",
    "    return syn_train, train_label, syn_test, test_label\n",
    "\n",
    "\n",
    "def load_usps(base_path):\n",
    "    print(\"load usps\")\n",
    "    usps_dataset = loadmat(path.join(base_path, \"usps_28x28.mat\"))\n",
    "    usps_dataset = usps_dataset[\"dataset\"]\n",
    "    usps_train = usps_dataset[0][0]\n",
    "    train_label = usps_dataset[0][1]\n",
    "    train_label = train_label.reshape(-1)\n",
    "    train_label[train_label == 10] = 0\n",
    "    usps_test = usps_dataset[1][0]\n",
    "    test_label = usps_dataset[1][1]\n",
    "    test_label = test_label.reshape(-1)\n",
    "    test_label[test_label == 10] = 0\n",
    "    usps_train = usps_train * 255\n",
    "    usps_test = usps_test * 255\n",
    "    usps_train = np.concatenate([usps_train, usps_train, usps_train], 1)\n",
    "   # print(usps_train.shape)\n",
    "    usps_train = np.tile(usps_train, (4, 1, 1, 1))\n",
    "    train_label = np.tile(train_label,4)\n",
    "    usps_train = usps_train[:25000]\n",
    "    train_label = train_label[:25000]\n",
    "    usps_test = np.concatenate([usps_test, usps_test, usps_test], 1)\n",
    "    #print('usps',usps_train.shape,train_label.shape, usps_test.shape,test_label.shape)\n",
    "    return usps_train, train_label, usps_test, test_label\n",
    "\n",
    "class Digit5Dataset(data.Dataset):\n",
    "    def __init__(self, data, labels, transform=None, target_transform=None):\n",
    "        super(Digit5Dataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index], self.labels[index]\n",
    "        if img.shape[0] != 1:\n",
    "            # transpose to Image type,so that the transform function can be used\n",
    "            img = Image.fromarray(np.uint8(np.asarray(img.transpose((1, 2, 0)))))\n",
    "\n",
    "        elif img.shape[0] == 1:\n",
    "            im = np.uint8(np.asarray(img))\n",
    "            # turn the raw image into 3 channels\n",
    "            im = np.vstack([im, im, im]).transpose((1, 2, 0))\n",
    "            img = Image.fromarray(im)\n",
    "\n",
    "        # do transform with PIL\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "def digit5_dataset_read(base_path, domain):\n",
    "    if domain == \"mnist\":\n",
    "        train_image, train_label, test_image, test_label = load_mnist(base_path)\n",
    "    elif domain == \"mnistm\":\n",
    "        train_image, train_label, test_image, test_label = load_mnist_m(base_path)\n",
    "    elif domain == \"svhn\":\n",
    "        train_image, train_label, test_image, test_label = load_svhn(base_path)\n",
    "    elif domain == \"syn\":\n",
    "        train_image, train_label, test_image, test_label = load_syn(base_path)\n",
    "    elif domain == \"usps\":\n",
    "        train_image, train_label, test_image, test_label = load_usps(base_path)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Domain {} Not Implemented\".format(domain))\n",
    "    # define the transform function\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    # raise train and test data loader\n",
    "    train_dataset = Digit5Dataset(data=train_image, labels=train_label, transform=transform)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    test_dataset = Digit5Dataset(data=test_image, labels=test_label, transform=transform)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "# Allocate data to usersz``\n",
    "def generate_Digit5(dir_path, part_type='standard'):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "# 设置目录路径\n",
    "    config_path = dir_path + \"config.json\"\n",
    "    train_path = dir_path + \"train/\"\n",
    "    test_path = dir_path + \"test/\"\n",
    "    # config_path、train_path和test_path分别表示配置文件、训练数据和测试数据的存储路径。\n",
    "    \n",
    "# 创建目录 如果训练或测试数据的目录不存在，代码会创建这些目录。\n",
    "    if not os.path.exists(train_path):\n",
    "        os.makedirs(train_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "# 初始化变量\n",
    "    root = dir_path + \"rawdata\" \n",
    "    #`root`变量代表原始数据的路径。  \n",
    "    X, y = [], []\n",
    "    # `X`和`y`是空的列表，用于存储从各个数据源读取的图像和标签数据。  \n",
    "    domains = ['mnistm', 'mnist', 'syn', 'usps', 'svhn']\n",
    "    # `domains`是一个列表，包含要从中读取数据的各个数据源的名称。\n",
    "\n",
    "# 读取数据\n",
    "    for d in domains:\n",
    "    # 对于domains列表中的每一个数据源d，代码使用digit5_dataset_read函数来读取训练和测试数据。\n",
    "        train_loader, test_loader = digit5_dataset_read(root, d)\n",
    "# 处理数据\n",
    "        for _, tt in enumerate(train_loader):\n",
    "        # 代码遍历`train_loader`和`test_loader`中的每一个数据批次。  \n",
    "            train_data, train_label = tt\n",
    "        for _, tt in enumerate(test_loader):\n",
    "            test_data, test_label = tt\n",
    "\n",
    "        dataset_image = []\n",
    "        dataset_label = []\n",
    "# 对于每个批次，它提取数据和标签，并将它们从PyTorch张量转换为NumPy数组。\n",
    "# 这些图像和标签被分别添加到`dataset_image`和`dataset_label`列表中。  \n",
    "        dataset_image.extend(train_data.cpu().detach().numpy())\n",
    "        dataset_image.extend(test_data.cpu().detach().numpy())\n",
    "        dataset_label.extend(train_label.cpu().detach().numpy())\n",
    "        dataset_label.extend(test_label.cpu().detach().numpy())\n",
    "# 最后，`dataset_image`和`dataset_label`被转换为NumPy数组，并分别添加到`X`和`y`列表中。\n",
    "        X.append(np.array(dataset_image))\n",
    "        y.append(np.array(dataset_label))\n",
    "# 数据合并与随机打乱\n",
    "    if part_type == 'shuffled':\n",
    "        # Combine all datasets\n",
    "        combined_X = np.concatenate(X)\n",
    "        combined_y = np.concatenate(y)\n",
    "        # Shuffle datasets 为了在后续的模型训练或评估中引入随机性，以减小数据顺序对结果的影响。\n",
    "        combined_X, combined_y = shuffle(combined_X, combined_y, random_state=0)\n",
    "        # 打乱后的数据集combined_X和combined_y会被分割成与数据源数量（len(domains)）相等的部分，每个部分代表一个数据源的数据。\n",
    "        split_X = np.array_split(combined_X, len(domains))\n",
    "        split_y = np.array_split(combined_y, len(domains))\n",
    "        X = split_X\n",
    "        y = split_y\n",
    "\n",
    "# 代码计算每个数据源的标签种类数量（即每个数据源有多少种不同的标签）\n",
    "# 并将这些数量存储在labelss列表中。同时，num_clients变量记录了数据源的数量，也就是客户端的数量。\n",
    "    labelss = []\n",
    "    for yy in y:\n",
    "        labelss.append(len(set(yy)))\n",
    "    num_clients = len(y)\n",
    "    print(f'Number of labels: {labelss}')\n",
    "    print(f'Number of clients: {num_clients}')\n",
    "# 对于每个数据源（客户端），代码统计了每个标签出现的次数\n",
    "# 并将结果以(label, count)的形式存储在statistic列表中。这有助于了解每个数据源的标签分布情况。\n",
    "    statistic = [[] for _ in range(num_clients)]\n",
    "    for client in range(num_clients):\n",
    "        for i in np.unique(y[client]):\n",
    "            statistic[client].append((int(i), int(sum(y[client] == i))))\n",
    "# 使用split_data函数将合并后的数据集X和y分割成训练数据和测试数据。使用save_file函数将训练数据、\n",
    "#测试数据、客户端数量、最大标签数量以及标签分布统计等信息保存到指定的配置路径、训练路径和测试路径中。\n",
    "    train_data, test_data = split_data(X, y)\n",
    "    save_file(config_path, train_path, test_path, train_data, test_data, num_clients, max(labelss), \n",
    "              statistic, None, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class FedAvgCNN(nn.Module):\n",
    "    def __init__(self, in_features=1, num_classes=10, dim=1024):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features,\n",
    "                        32,\n",
    "                        kernel_size=5,\n",
    "                        padding=0,\n",
    "                        stride=1,\n",
    "                        bias=True),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32,\n",
    "                        64,\n",
    "                        kernel_size=5,\n",
    "                        padding=0,\n",
    "                        stride=1,\n",
    "                        bias=True),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(dim, 512), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class Digit5CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Digit5CNN, self).__init__()\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.add_module(\"conv1\", nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2))\n",
    "        self.encoder.add_module(\"bn1\", nn.BatchNorm2d(64))\n",
    "        self.encoder.add_module(\"relu1\", nn.ReLU())\n",
    "        self.encoder.add_module(\"maxpool1\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False))\n",
    "        self.encoder.add_module(\"conv2\", nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2))\n",
    "        self.encoder.add_module(\"bn2\", nn.BatchNorm2d(64))\n",
    "        self.encoder.add_module(\"relu2\", nn.ReLU())\n",
    "        self.encoder.add_module(\"maxpool2\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False))\n",
    "        self.encoder.add_module(\"conv3\", nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2))\n",
    "        self.encoder.add_module(\"bn3\", nn.BatchNorm2d(128))\n",
    "        self.encoder.add_module(\"relu3\", nn.ReLU())\n",
    "\n",
    "        self.linear = nn.Sequential()\n",
    "        self.linear.add_module(\"fc1\", nn.Linear(8192, 3072))\n",
    "        self.linear.add_module(\"bn4\", nn.BatchNorm1d(3072))\n",
    "        self.linear.add_module(\"relu4\", nn.ReLU())\n",
    "        self.linear.add_module(\"dropout\", nn.Dropout())\n",
    "        self.linear.add_module(\"fc2\", nn.Linear(3072, 2048))\n",
    "        self.linear.add_module(\"bn5\", nn.BatchNorm1d(2048))\n",
    "        self.linear.add_module(\"relu5\", nn.ReLU())\n",
    "\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        feature = self.encoder(x)\n",
    "        feature = feature.view(batch_size, -1)\n",
    "        feature = self.linear(feature)\n",
    "        out = self.fc(feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/data_utils.py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def read_data(dataset, idx, is_train=True):\n",
    "    if is_train:\n",
    "        train_data_dir = os.path.join('./dataset', dataset, 'train/')\n",
    "\n",
    "        train_file = train_data_dir + str(idx) + '.npz'\n",
    "        with open(train_file, 'rb') as f:\n",
    "            train_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "\n",
    "        return train_data\n",
    "\n",
    "    else:\n",
    "        test_data_dir = os.path.join('./dataset', dataset, 'test/')\n",
    "\n",
    "        test_file = test_data_dir + str(idx) + '.npz'\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "\n",
    "        return test_data\n",
    "\n",
    "\n",
    "def read_client_data(dataset, idx, is_train=True):\n",
    "    if \"News\" in dataset:\n",
    "        return read_client_data_text(dataset, idx, is_train)\n",
    "    elif \"Shakespeare\" in dataset:\n",
    "        return read_client_data_Shakespeare(dataset, idx)\n",
    "\n",
    "    if is_train:\n",
    "        train_data = read_data(dataset, idx, is_train)\n",
    "        X_train = torch.Tensor(train_data['x']).type(torch.float32)\n",
    "        y_train = torch.Tensor(train_data['y']).type(torch.int64)\n",
    "\n",
    "        train_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "        return train_data\n",
    "    else:\n",
    "        test_data = read_data(dataset, idx, is_train)\n",
    "        X_test = torch.Tensor(test_data['x']).type(torch.float32)\n",
    "        y_test = torch.Tensor(test_data['y']).type(torch.int64)\n",
    "        test_data = [(x, y) for x, y in zip(X_test, y_test)]\n",
    "        return test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client base\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class Client(object):\n",
    "    \"\"\"\n",
    "    Base class for clients in federated learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, id, train_samples, test_samples, **kwargs):\n",
    "        torch.manual_seed(0)\n",
    "        self.model = copy.deepcopy(args.model)\n",
    "        self.algorithm = args.algorithm\n",
    "        self.dataset = args.dataset\n",
    "        self.device = args.device\n",
    "        self.id = id  # integer\n",
    "        self.save_folder_name = args.save_folder_name\n",
    "\n",
    "        self.num_classes = args.num_classes\n",
    "        self.train_samples = train_samples\n",
    "        self.test_samples = test_samples\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.local_learning_rate\n",
    "        self.local_epochs = args.local_epochs\n",
    "\n",
    "        # check BatchNorm\n",
    "        self.has_BatchNorm = False\n",
    "        for layer in self.model.children():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                self.has_BatchNorm = True\n",
    "                break\n",
    "\n",
    "        self.train_slow = kwargs['train_slow']\n",
    "        self.send_slow = kwargs['send_slow']\n",
    "        self.train_time_cost = {'num_rounds': 0, 'total_cost': 0.0}\n",
    "        self.send_time_cost = {'num_rounds': 0, 'total_cost': 0.0}\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.learning_rate_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer=self.optimizer, \n",
    "            gamma=args.learning_rate_decay_gamma\n",
    "        )\n",
    "        self.learning_rate_decay = args.learning_rate_decay\n",
    "\n",
    "\n",
    "    def load_train_data(self, batch_size=None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "        train_data = read_client_data(self.dataset, self.id, is_train=True)\n",
    "        return DataLoader(train_data, batch_size, drop_last=True, shuffle=True)\n",
    "\n",
    "    def load_test_data(self, batch_size=None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "        test_data = read_client_data(self.dataset, self.id, is_train=False)\n",
    "        return DataLoader(test_data, batch_size, drop_last=False, shuffle=True)\n",
    "        \n",
    "    def set_parameters(self, model):\n",
    "        for new_param, old_param in zip(model.parameters(), self.model.parameters()):\n",
    "            old_param.data = new_param.data.clone()\n",
    "\n",
    "    def clone_model(self, model, target):\n",
    "        for param, target_param in zip(model.parameters(), target.parameters()):\n",
    "            target_param.data = param.data.clone()\n",
    "            # target_param.grad = param.grad.clone()\n",
    "\n",
    "    def update_parameters(self, model, new_params):\n",
    "        for param, new_param in zip(model.parameters(), new_params):\n",
    "            param.data = new_param.data.clone()\n",
    "\n",
    "    def test_metrics(self):\n",
    "        testloaderfull = self.load_test_data()\n",
    "        self.model.eval()\n",
    "\n",
    "        test_acc = 0\n",
    "        test_num = 0\n",
    "        y_prob = []\n",
    "        y_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in testloaderfull:\n",
    "                if type(x) == type([]):\n",
    "                    x[0] = x[0].to(self.device)\n",
    "                else:\n",
    "                    x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                output = self.model(x)\n",
    "\n",
    "                test_acc += (torch.sum(torch.argmax(output, dim=1) == y)).item()\n",
    "                test_num += y.shape[0]\n",
    "\n",
    "                y_prob.append(output.detach().cpu().numpy())\n",
    "                nc = self.num_classes\n",
    "                if self.num_classes == 2:\n",
    "                    nc += 1\n",
    "                lb = label_binarize(y.detach().cpu().numpy(), classes=np.arange(nc))\n",
    "                if self.num_classes == 2:\n",
    "                    lb = lb[:, :2]\n",
    "                y_true.append(lb)\n",
    "\n",
    "\n",
    "        y_prob = np.concatenate(y_prob, axis=0)\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_true, y_prob, average='micro')\n",
    "        \n",
    "        return test_acc, test_num, auc\n",
    "\n",
    "    def train_metrics(self):\n",
    "        trainloader = self.load_train_data()\n",
    "        self.model.eval()\n",
    "\n",
    "        train_num = 0\n",
    "        losses = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in trainloader:\n",
    "                if type(x) == type([]):\n",
    "                    x[0] = x[0].to(self.device)\n",
    "                else:\n",
    "                    x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                output = self.model(x)\n",
    "                loss = self.loss(output, y)\n",
    "                train_num += y.shape[0]\n",
    "                losses += loss.item() * y.shape[0]\n",
    "\n",
    "\n",
    "        return losses, train_num\n",
    "\n",
    "\n",
    "    def save_item(self, item, item_name, item_path=None):\n",
    "        if item_path == None:\n",
    "            item_path = self.save_folder_name\n",
    "        if not os.path.exists(item_path):\n",
    "            os.makedirs(item_path)\n",
    "        torch.save(item, os.path.join(item_path, \"client_\" + str(self.id) + \"_\" + item_name + \".pt\"))\n",
    "\n",
    "    def load_item(self, item_name, item_path=None):\n",
    "        if item_path == None:\n",
    "            item_path = self.save_folder_name\n",
    "        return torch.load(os.path.join(item_path, \"client_\" + str(self.id) + \"_\" + item_name + \".pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class Server(object):\n",
    "    def __init__(self, args, times):\n",
    "        # Set up the main attributes\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "        self.dataset = args.dataset\n",
    "        self.num_classes = args.num_classes\n",
    "        self.global_rounds = args.global_rounds\n",
    "        self.local_epochs = args.local_epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.local_learning_rate\n",
    "        self.global_model = copy.deepcopy(args.model)\n",
    "        self.num_clients = args.num_clients\n",
    "        self.join_ratio = args.join_ratio\n",
    "        self.random_join_ratio = args.random_join_ratio\n",
    "        self.num_join_clients = int(self.num_clients * self.join_ratio)\n",
    "        self.current_num_join_clients = self.num_join_clients\n",
    "        self.algorithm = args.algorithm\n",
    "        self.time_select = args.time_select\n",
    "        self.goal = args.goal\n",
    "        self.time_threthold = args.time_threthold\n",
    "        self.save_folder_name = args.save_folder_name\n",
    "        self.top_cnt = 100\n",
    "\n",
    "        self.clients = []\n",
    "        self.selected_clients = []\n",
    "        self.train_slow_clients = []\n",
    "        self.send_slow_clients = []\n",
    "\n",
    "        self.uploaded_weights = []\n",
    "        self.uploaded_ids = []\n",
    "        self.uploaded_models = []\n",
    "\n",
    "        self.rs_test_acc = []\n",
    "        self.rs_test_auc = []\n",
    "        self.rs_train_loss = []\n",
    "\n",
    "        self.times = times\n",
    "        self.eval_gap = args.eval_gap\n",
    "        self.client_drop_rate = args.client_drop_rate\n",
    "        self.train_slow_rate = args.train_slow_rate\n",
    "        self.send_slow_rate = args.send_slow_rate\n",
    "\n",
    "        self.batch_num_per_client = args.batch_num_per_client\n",
    "\n",
    "        self.num_new_clients = args.num_new_clients\n",
    "        self.new_clients = []\n",
    "        self.eval_new_clients = False\n",
    "        self.fine_tuning_epoch_new = args.fine_tuning_epoch_new\n",
    "\n",
    "    def set_clients(self, clientObj):\n",
    "        for i, train_slow, send_slow in zip(range(self.num_clients), self.train_slow_clients, self.send_slow_clients):\n",
    "            train_data = read_client_data(self.dataset, i, is_train=True)\n",
    "            test_data = read_client_data(self.dataset, i, is_train=False)\n",
    "            client = clientObj(self.args, \n",
    "                            id=i, \n",
    "                            train_samples=len(train_data), \n",
    "                            test_samples=len(test_data), \n",
    "                            train_slow=train_slow, \n",
    "                            send_slow=send_slow)\n",
    "            self.clients.append(client)\n",
    "\n",
    "    # random select slow clients\n",
    "    def select_slow_clients(self, slow_rate):\n",
    "        slow_clients = [False for i in range(self.num_clients)]\n",
    "        idx = [i for i in range(self.num_clients)]\n",
    "        idx_ = np.random.choice(idx, int(slow_rate * self.num_clients))\n",
    "        for i in idx_:\n",
    "            slow_clients[i] = True\n",
    "\n",
    "        return slow_clients\n",
    "\n",
    "    def set_slow_clients(self):\n",
    "        self.train_slow_clients = self.select_slow_clients(\n",
    "            self.train_slow_rate)\n",
    "        self.send_slow_clients = self.select_slow_clients(\n",
    "            self.send_slow_rate)\n",
    "\n",
    "    def select_clients(self):\n",
    "        if self.random_join_ratio:\n",
    "            self.current_num_join_clients = np.random.choice(range(self.num_join_clients, self.num_clients+1), 1, replace=False)[0]\n",
    "        else:\n",
    "            self.current_num_join_clients = self.num_join_clients\n",
    "        selected_clients = list(np.random.choice(self.clients, self.current_num_join_clients, replace=False))\n",
    "\n",
    "        return selected_clients\n",
    "\n",
    "    def send_models(self):\n",
    "        assert (len(self.clients) > 0)\n",
    "\n",
    "        for client in self.clients:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            client.set_parameters(self.global_model)\n",
    "\n",
    "            client.send_time_cost['num_rounds'] += 1\n",
    "            client.send_time_cost['total_cost'] += 2 * (time.time() - start_time)\n",
    "\n",
    "    def receive_models(self):\n",
    "        assert (len(self.selected_clients) > 0)\n",
    "\n",
    "        active_clients = random.sample(\n",
    "            self.selected_clients, int((1-self.client_drop_rate) * self.current_num_join_clients))\n",
    "\n",
    "        self.uploaded_ids = []\n",
    "        self.uploaded_weights = []\n",
    "        self.uploaded_models = []\n",
    "        tot_samples = 0\n",
    "        for client in active_clients:\n",
    "            try:\n",
    "                client_time_cost = client.train_time_cost['total_cost'] / client.train_time_cost['num_rounds'] + \\\n",
    "                        client.send_time_cost['total_cost'] / client.send_time_cost['num_rounds']\n",
    "            except ZeroDivisionError:\n",
    "                client_time_cost = 0\n",
    "            if client_time_cost <= self.time_threthold:\n",
    "                tot_samples += client.train_samples\n",
    "                self.uploaded_ids.append(client.id)\n",
    "                self.uploaded_weights.append(client.train_samples)\n",
    "                self.uploaded_models.append(client.model)\n",
    "        for i, w in enumerate(self.uploaded_weights):\n",
    "            self.uploaded_weights[i] = w / tot_samples\n",
    "\n",
    "    def aggregate_parameters(self):\n",
    "        assert (len(self.uploaded_models) > 0)\n",
    "\n",
    "        self.global_model = copy.deepcopy(self.uploaded_models[0])\n",
    "        for param in self.global_model.parameters():\n",
    "            param.data.zero_()\n",
    "            \n",
    "        for w, client_model in zip(self.uploaded_weights, self.uploaded_models):\n",
    "            self.add_parameters(w, client_model)\n",
    "\n",
    "    def add_parameters(self, w, client_model):\n",
    "        for server_param, client_param in zip(self.global_model.parameters(), client_model.parameters()):\n",
    "            server_param.data += client_param.data.clone() * w\n",
    "\n",
    "    def save_global_model(self):\n",
    "        model_path = os.path.join(\"models\", self.dataset)\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "        model_path = os.path.join(model_path, self.algorithm + \"_server\" + \".pt\")\n",
    "        torch.save(self.global_model, model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        model_path = os.path.join(\"models\", self.dataset)\n",
    "        model_path = os.path.join(model_path, self.algorithm + \"_server\" + \".pt\")\n",
    "        assert (os.path.exists(model_path))\n",
    "        self.global_model = torch.load(model_path)\n",
    "\n",
    "    def model_exists(self):\n",
    "        model_path = os.path.join(\"models\", self.dataset)\n",
    "        model_path = os.path.join(model_path, self.algorithm + \"_server\" + \".pt\")\n",
    "        return os.path.exists(model_path)\n",
    "        \n",
    "    def save_results(self):\n",
    "        algo = self.dataset + \"_\" + self.algorithm\n",
    "        result_path = \"./results/\"\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        if (len(self.rs_test_acc)):\n",
    "            algo = algo + \"_\" + self.goal + \"_\" + str(self.times)\n",
    "            file_path = result_path + \"{}.h5\".format(algo)\n",
    "            print(\"File path: \" + file_path)\n",
    "\n",
    "            with h5py.File(file_path, 'w') as hf:\n",
    "                hf.create_dataset('rs_test_acc', data=self.rs_test_acc)\n",
    "                hf.create_dataset('rs_test_auc', data=self.rs_test_auc)\n",
    "                hf.create_dataset('rs_train_loss', data=self.rs_train_loss)\n",
    "\n",
    "    def save_item(self, item, item_name):\n",
    "        if not os.path.exists(self.save_folder_name):\n",
    "            os.makedirs(self.save_folder_name)\n",
    "        torch.save(item, os.path.join(self.save_folder_name, \"server_\" + item_name + \".pt\"))\n",
    "\n",
    "    def load_item(self, item_name):\n",
    "        return torch.load(os.path.join(self.save_folder_name, \"server_\" + item_name + \".pt\"))\n",
    "\n",
    "    def test_metrics(self):\n",
    "        if self.eval_new_clients and self.num_new_clients > 0:\n",
    "            self.fine_tuning_new_clients()\n",
    "            return self.test_metrics_new_clients()\n",
    "        \n",
    "        num_samples = []\n",
    "        tot_correct = []\n",
    "        tot_auc = []\n",
    "        for c in self.clients:\n",
    "            ct, ns, auc = c.test_metrics()\n",
    "            tot_correct.append(ct*1.0)\n",
    "            tot_auc.append(auc*ns)\n",
    "            num_samples.append(ns)\n",
    "\n",
    "        ids = [c.id for c in self.clients]\n",
    "\n",
    "        return ids, num_samples, tot_correct, tot_auc\n",
    "\n",
    "    def train_metrics(self):\n",
    "        if self.eval_new_clients and self.num_new_clients > 0:\n",
    "            return [0], [1], [0]\n",
    "        \n",
    "        num_samples = []\n",
    "        losses = []\n",
    "        for c in self.clients:\n",
    "            cl, ns = c.train_metrics()\n",
    "            num_samples.append(ns)\n",
    "            losses.append(cl*1.0)\n",
    "\n",
    "        ids = [c.id for c in self.clients]\n",
    "\n",
    "        return ids, num_samples, losses\n",
    "\n",
    "    # evaluate selected clients\n",
    "    def evaluate(self, acc=None, loss=None):\n",
    "        stats = self.test_metrics()\n",
    "        stats_train = self.train_metrics()\n",
    "\n",
    "        test_acc = sum(stats[2])*1.0 / sum(stats[1])\n",
    "        test_auc = sum(stats[3])*1.0 / sum(stats[1])\n",
    "        train_loss = sum(stats_train[2])*1.0 / sum(stats_train[1])\n",
    "        accs = [a / n for a, n in zip(stats[2], stats[1])]\n",
    "        aucs = [a / n for a, n in zip(stats[3], stats[1])]\n",
    "        \n",
    "        if acc == None:\n",
    "            self.rs_test_acc.append(test_acc)\n",
    "        else:\n",
    "            acc.append(test_acc)\n",
    "        \n",
    "        if loss == None:\n",
    "            self.rs_train_loss.append(train_loss)\n",
    "        else:\n",
    "            loss.append(train_loss)\n",
    "\n",
    "        print(\"Averaged Train Loss: {:.4f}\".format(train_loss))\n",
    "        print(\"Averaged Test Accuracy: {:.4f}\".format(test_acc))\n",
    "        print(\"Averaged Test AUC: {:.4f}\".format(test_auc))\n",
    "        # self.print_(test_acc, train_acc, train_loss)\n",
    "        print(\"Std Test Accuracy: {:.4f}\".format(np.std(accs)))\n",
    "        print(\"Std Test AUC: {:.4f}\".format(np.std(aucs)))\n",
    "\n",
    "    def print_(self, test_acc, test_auc, train_loss):\n",
    "        print(\"Average Test Accuracy: {:.4f}\".format(test_acc))\n",
    "        print(\"Average Test AUC: {:.4f}\".format(test_auc))\n",
    "        print(\"Average Train Loss: {:.4f}\".format(train_loss))\n",
    "\n",
    "    def check_done(self, acc_lss, top_cnt=None, div_value=None):\n",
    "        for acc_ls in acc_lss:\n",
    "            if top_cnt != None and div_value != None:\n",
    "                find_top = len(acc_ls) - torch.topk(torch.tensor(acc_ls), 1).indices[0] > top_cnt\n",
    "                find_div = len(acc_ls) > 1 and np.std(acc_ls[-top_cnt:]) < div_value\n",
    "                if find_top and find_div:\n",
    "                    pass\n",
    "                else:\n",
    "                    return False\n",
    "            elif top_cnt != None:\n",
    "                find_top = len(acc_ls) - torch.topk(torch.tensor(acc_ls), 1).indices[0] > top_cnt\n",
    "                if find_top:\n",
    "                    pass\n",
    "                else:\n",
    "                    return False\n",
    "            elif div_value != None:\n",
    "                find_div = len(acc_ls) > 1 and np.std(acc_ls[-top_cnt:]) < div_value\n",
    "                if find_div:\n",
    "                    pass\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        return True\n",
    "\n",
    "\n",
    "    def set_new_clients(self, clientObj):\n",
    "        for i in range(self.num_clients, self.num_clients + self.num_new_clients):\n",
    "            train_data = read_client_data(self.dataset, i, is_train=True)\n",
    "            test_data = read_client_data(self.dataset, i, is_train=False)\n",
    "            client = clientObj(self.args, \n",
    "                            id=i, \n",
    "                            train_samples=len(train_data), \n",
    "                            test_samples=len(test_data), \n",
    "                            train_slow=False, \n",
    "                            send_slow=False)\n",
    "            self.new_clients.append(client)\n",
    "\n",
    "    # fine-tuning on new clients\n",
    "    def fine_tuning_new_clients(self):\n",
    "        for client in self.new_clients:\n",
    "            client.set_parameters(self.global_model)\n",
    "            opt = torch.optim.SGD(client.model.parameters(), lr=self.learning_rate)\n",
    "            CEloss = torch.nn.CrossEntropyLoss()\n",
    "            trainloader = client.load_train_data()\n",
    "            client.model.train()\n",
    "            for e in range(self.fine_tuning_epoch_new):\n",
    "                for i, (x, y) in enumerate(trainloader):\n",
    "                    if type(x) == type([]):\n",
    "                        x[0] = x[0].to(client.device)\n",
    "                    else:\n",
    "                        x = x.to(client.device)\n",
    "                    y = y.to(client.device)\n",
    "                    output = client.model(x)\n",
    "                    loss = CEloss(output, y)\n",
    "                    opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "    # evaluating on new clients\n",
    "    def test_metrics_new_clients(self):\n",
    "        num_samples = []\n",
    "        tot_correct = []\n",
    "        tot_auc = []\n",
    "        for c in self.new_clients:\n",
    "            ct, ns, auc = c.test_metrics()\n",
    "            tot_correct.append(ct*1.0)\n",
    "            tot_auc.append(auc*ns)\n",
    "            num_samples.append(ns)\n",
    "\n",
    "        ids = [c.id for c in self.new_clients]\n",
    "\n",
    "        return ids, num_samples, tot_correct, tot_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class clientAVG(Client):\n",
    "    def __init__(self, args, id, train_samples, test_samples, **kwargs):\n",
    "        super().__init__(args, id, train_samples, test_samples, **kwargs)\n",
    "\n",
    "    def train(self):\n",
    "        trainloader = self.load_train_data()\n",
    "        # self.model.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        max_local_epochs = self.local_epochs\n",
    "        if self.train_slow:\n",
    "            max_local_epochs = np.random.randint(1, max_local_epochs // 2)\n",
    "\n",
    "        for epoch in range(max_local_epochs):\n",
    "            for i, (x, y) in enumerate(trainloader):\n",
    "                if type(x) == type([]):\n",
    "                    x[0] = x[0].to(self.device)\n",
    "                else:\n",
    "                    x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                if self.train_slow:\n",
    "                    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "                output = self.model(x)\n",
    "                loss = self.loss(output, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # self.model.cpu()\n",
    "\n",
    "        if self.learning_rate_decay:\n",
    "            self.learning_rate_scheduler.step()\n",
    "\n",
    "        self.train_time_cost['num_rounds'] += 1\n",
    "        self.train_time_cost['total_cost'] += time.time() - start_time\n",
    "\n",
    "\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class FedAvg(Server):\n",
    "    def __init__(self, args, times):\n",
    "        super().__init__(args, times)\n",
    "\n",
    "        # select slow clients\n",
    "        self.set_slow_clients()\n",
    "        self.set_clients(clientAVG)\n",
    "\n",
    "        print(f\"\\nJoin ratio / total clients: {self.join_ratio} / {self.num_clients}\")\n",
    "        print(\"Finished creating server and clients.\")\n",
    "\n",
    "        # self.load_model()\n",
    "        self.Budget = []\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.global_rounds+1):\n",
    "            s_t = time.time()\n",
    "            self.selected_clients = self.select_clients()\n",
    "            self.send_models()\n",
    "\n",
    "            if i%self.eval_gap == 0:\n",
    "                print(f\"\\n-------------Round number: {i}-------------\")\n",
    "                print(\"\\nEvaluate global model\")\n",
    "                self.evaluate()\n",
    "\n",
    "            for client in self.selected_clients:\n",
    "                client.train()\n",
    "\n",
    "            # threads = [Thread(target=client.train)\n",
    "            #            for client in self.selected_clients]\n",
    "            # [t.start() for t in threads]\n",
    "            # [t.join() for t in threads]\n",
    "\n",
    "            self.receive_models()\n",
    " \n",
    "            self.aggregate_parameters()\n",
    "\n",
    "            self.Budget.append(time.time() - s_t)\n",
    "            print('-'*25, 'time cost', '-'*25, self.Budget[-1])\n",
    "\n",
    "\n",
    "        print(\"\\nBest accuracy.\")\n",
    "        # self.print_(max(self.rs_test_acc), max(\n",
    "        #     self.rs_train_acc), min(self.rs_train_loss))\n",
    "        print(max(self.rs_test_acc))\n",
    "        print(\"\\nAverage time cost per round.\")\n",
    "        print(sum(self.Budget[1:])/len(self.Budget[1:]))\n",
    "\n",
    "        self.save_results()\n",
    "        self.save_global_model()\n",
    "\n",
    "        if self.num_new_clients > 0:\n",
    "            self.eval_new_clients = True\n",
    "            self.set_new_clients(clientAVG)\n",
    "            print(f\"\\n-------------Fine tuning round-------------\")\n",
    "            print(\"\\nEvaluate new clients\")\n",
    "            self.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedamp\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "class clientAMP(Client):\n",
    "    def __init__(self, args, id, train_samples, test_samples, **kwargs):\n",
    "        super().__init__(args, id, train_samples, test_samples, **kwargs)\n",
    "        \n",
    "        self.alphaK = args.alphaK\n",
    "        self.lamda = args.lamda\n",
    "        self.client_u = copy.deepcopy(self.model)\n",
    "\n",
    "    def train(self):\n",
    "        trainloader = self.load_train_data()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        max_local_epochs = self.local_epochs\n",
    "        if self.train_slow:\n",
    "            max_local_epochs = np.random.randint(1, max_local_epochs // 2)\n",
    "\n",
    "        for epoch in range(max_local_epochs):\n",
    "            for x, y in trainloader:\n",
    "                if type(x) == type([]):\n",
    "                    x[0] = x[0].to(self.device)\n",
    "                else:\n",
    "                    x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                if self.train_slow:\n",
    "                    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "                output = self.model(x)\n",
    "                loss = self.loss(output, y)\n",
    "\n",
    "                gm = torch.cat([p.data.view(-1) for p in self.model.parameters()], dim=0)\n",
    "                pm = torch.cat([p.data.view(-1) for p in self.client_u.parameters()], dim=0)\n",
    "                loss += 0.5 * self.lamda/self.alphaK * torch.norm(gm-pm, p=2)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # self.model.cpu()\n",
    "\n",
    "        if self.learning_rate_decay:\n",
    "            self.learning_rate_scheduler.step()\n",
    "\n",
    "        self.train_time_cost['num_rounds'] += 1\n",
    "        self.train_time_cost['total_cost'] += time.time() - start_time\n",
    "\n",
    "\n",
    "    def set_parameters(self, model, coef_self):\n",
    "        for new_param, old_param, self_param in zip(model.parameters(), self.client_u.parameters(), self.model.parameters()):\n",
    "            old_param.data = (new_param.data + coef_self * self_param.data).clone()\n",
    "\n",
    "\n",
    "    def train_metrics(self, model=None):\n",
    "        trainloader = self.load_train_data()\n",
    "        if model == None:\n",
    "            model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        train_num = 0\n",
    "        losses = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in trainloader:\n",
    "                if type(x) == type([]):\n",
    "                    x[0] = x[0].to(self.device)\n",
    "                else:\n",
    "                    x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                output = self.model(x)\n",
    "                loss = self.loss(output, y)\n",
    "\n",
    "                gm = torch.cat([p.data.view(-1) for p in self.model.parameters()], dim=0)\n",
    "                pm = torch.cat([p.data.view(-1) for p in self.client_u.parameters()], dim=0)\n",
    "                loss += 0.5 * self.lamda/self.alphaK * torch.norm(gm-pm, p=2)\n",
    "\n",
    "                train_num += y.shape[0]\n",
    "                losses += loss.item() * y.shape[0]\n",
    "\n",
    "        return losses, train_num\n",
    "\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from threading import Thread\n",
    "\n",
    "class FedAMP(Server):\n",
    "    def __init__(self, args, times):\n",
    "        super().__init__(args, times)\n",
    "\n",
    "        # select slow clients\n",
    "        self.set_slow_clients()\n",
    "        self.set_clients(clientAMP)\n",
    "\n",
    "        self.alphaK = args.alphaK\n",
    "        self.sigma = args.sigma\n",
    "\n",
    "        print(f\"\\nJoin ratio / total clients: {self.join_ratio} / {self.num_clients}\")\n",
    "        print(\"Finished creating server and clients.\")\n",
    "        self.Budget = []\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.global_rounds+1):\n",
    "            s_t = time.time()\n",
    "            self.selected_clients = self.select_clients()\n",
    "            self.send_models()\n",
    "\n",
    "            if i%self.eval_gap == 0:\n",
    "                print(f\"\\n-------------Round number: {i}-------------\")\n",
    "                print(\"\\nEvaluate global model\")\n",
    "                self.evaluate()\n",
    "\n",
    "            for client in self.selected_clients:\n",
    "                client.train()\n",
    "\n",
    "\n",
    "            self.receive_models()\n",
    "\n",
    "            self.Budget.append(time.time() - s_t)\n",
    "            print('-'*25, 'time cost', '-'*25, self.Budget[-1])\n",
    "\n",
    "        print(\"\\nBest accuracy.\")\n",
    "        print(max(self.rs_test_acc))\n",
    "        print(\"\\nAverage time cost per round.\")\n",
    "        print(sum(self.Budget[1:])/len(self.Budget[1:]))\n",
    "\n",
    "        self.save_results()\n",
    "\n",
    "\n",
    "    # To save GPU memory in simulation, no persistent model is kept on the server.\n",
    "    def send_models(self):\n",
    "        assert (len(self.selected_clients) > 0)\n",
    "\n",
    "        if len(self.uploaded_ids) > 0:\n",
    "            for c in self.selected_clients:\n",
    "                mu = copy.deepcopy(self.global_model)\n",
    "                for param in mu.parameters():\n",
    "                    param.data.zero_()\n",
    "\n",
    "                coef = torch.zeros(self.num_join_clients)\n",
    "                for j, mw in enumerate(self.uploaded_models):\n",
    "                    if c.id != self.uploaded_ids[j]:\n",
    "                        weights_i = torch.cat([p.data.view(-1) for p in c.model.parameters()], dim=0)\n",
    "                        weights_j = torch.cat([p.data.view(-1) for p in mw.parameters()], dim=0)\n",
    "                        sub = (weights_i - weights_j).view(-1)\n",
    "                        sub = torch.dot(sub, sub)\n",
    "                        coef[j] = self.alphaK * self.e(sub)\n",
    "                    else:\n",
    "                        coef[j] = 0\n",
    "                coef_self = 1 - torch.sum(coef)\n",
    "                # print(i, coef)\n",
    "\n",
    "                for j, mw in enumerate(self.uploaded_models):\n",
    "                    for param, param_j in zip(mu.parameters(), mw.parameters()):\n",
    "                        param.data += coef[j] * param_j\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                if c.send_slow:\n",
    "                    time.sleep(0.1 * np.abs(np.random.rand()))\n",
    "\n",
    "                c.set_parameters(mu, coef_self)\n",
    "\n",
    "                c.send_time_cost['num_rounds'] += 1\n",
    "                c.send_time_cost['total_cost'] += 2 * (time.time() - start_time)\n",
    "\n",
    "    def e(self, x):\n",
    "        return math.exp(-x/self.sigma)/self.sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import logging\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def average_data(algorithm=\"\", dataset=\"\", goal=\"\", times=10):\n",
    "    test_acc = get_all_results_for_one_algo(algorithm, dataset, goal, times)\n",
    "\n",
    "    max_accuracy = []\n",
    "    for i in range(times):\n",
    "        max_accuracy.append(test_acc[i].max())\n",
    "\n",
    "    print(\"std for best accuracy:\", np.std(max_accuracy))\n",
    "    print(\"mean for best accuracy:\", np.mean(max_accuracy))\n",
    "\n",
    "\n",
    "def get_all_results_for_one_algo(algorithm=\"\", dataset=\"\", goal=\"\", times=10):\n",
    "    test_acc = []\n",
    "    algorithms_list = [algorithm] * times\n",
    "    for i in range(times):\n",
    "        file_name = dataset + \"_\" + algorithms_list[i] + \"_\" + goal + \"_\" + str(i)\n",
    "        test_acc.append(np.array(read_data_then_delete(file_name, delete=False)))\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def read_data_then_delete(file_name, delete=False):\n",
    "    file_path = \"./results/\" + file_name + \".h5\"\n",
    "\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        rs_test_acc = np.array(hf.get('rs_test_acc'))\n",
    "\n",
    "    if delete:\n",
    "        os.remove(file_path)\n",
    "    print(\"Length: \", len(rs_test_acc))\n",
    "\n",
    "    return rs_test_acc\n",
    "\n",
    "\n",
    "\n",
    "def run(args):\n",
    "\n",
    "    time_list = []\n",
    "    model_str = args.model\n",
    "\n",
    "    for i in range(args.prev, args.times):\n",
    "        print(f\"\\n============= Running time: {i}th =============\")\n",
    "        print(\"Creating server and clients ...\")\n",
    "        start = time.time()\n",
    "\n",
    "\n",
    "        if model_str == \"cnn\": # non-convex\n",
    "            if \"mnist\" in args.dataset:\n",
    "                args.model = FedAvgCNN(in_features=1, num_classes=args.num_classes, dim=1024).to(args.device)\n",
    "            elif \"Cifar10\" in args.dataset:\n",
    "                args.model = FedAvgCNN(in_features=3, num_classes=args.num_classes, dim=1600).to(args.device)\n",
    "            elif \"Digit5\" in args.dataset:\n",
    "                args.model = Digit5CNN().to(args.device)\n",
    "\n",
    "        print(args.model)\n",
    "\n",
    "        # select algorithm\n",
    "        if args.algorithm == \"FedAvg\":\n",
    "            server = FedAvg(args, i)\n",
    "\n",
    "\n",
    "        elif args.algorithm == \"FedAMP\":\n",
    "            server = FedAMP(args, i)\n",
    "          \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        server.train()\n",
    "\n",
    "        time_list.append(time.time()-start)\n",
    "\n",
    "    print(f\"\\nAverage time cost: {round(np.average(time_list), 2)}s.\")\n",
    "    \n",
    "\n",
    "    # Global average\n",
    "    average_data(dataset=args.dataset, algorithm=args.algorithm, goal=args.goal, times=args.times)\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load mnist_m\n",
      "load mnist\n",
      "load syn\n",
      "load usps\n",
      "load svhn\n",
      "Number of labels: [10, 10, 10, 10, 10]\n",
      "Number of clients: 5\n",
      "Total number of samples: 162860\n",
      "The number of train samples: [24429, 24429, 24429, 24429, 24429]\n",
      "The number of test samples: [8143, 8143, 8143, 8143, 8143]\n",
      "\n",
      "Saving to disk.\n",
      "\n",
      "Finish generating dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset.py\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "num_clients = 20\n",
    "\n",
    "dataset = \"digit5\"\n",
    "#niid = True\n",
    "#balance = True\n",
    "# partition = \"dir\"\n",
    "part_type = \"shuffled\"  # 为noniid/shuffled为iid\n",
    "\n",
    "if dataset == \"mnist\":\n",
    "    dir_path = \"MNIST/\"\n",
    "    generate_mnist(dir_path, num_clients, niid, balance, partition)\n",
    "elif dataset == \"cifar10\":\n",
    "    dir_path = \"Cifar10/\"\n",
    "    generate_cifar10(dir_path, num_clients, niid, balance, partition)\n",
    "elif dataset == \"digit5\":\n",
    "    dir_path = \"./dataset/Digit5/\"\n",
    "    generate_Digit5(dir_path, part_type=part_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda is not available. Falling back to CPU.\n",
      "\n",
      "==================================================\n",
      "Algorithm: FedAvg\n",
      "Local batch size: 10\n",
      "Local epochs: 1\n",
      "Local learing rate: 0.005\n",
      "Local learing rate decay: False\n",
      "Total number of clients: 5\n",
      "Clients join in each round: 1.0\n",
      "Clients randomly join: False\n",
      "Client drop rate: 0.0\n",
      "Client select regarding time: False\n",
      "Running times: 1\n",
      "Dataset: Digit5\n",
      "Number of classes: 10\n",
      "Backbone: cnn\n",
      "Using device: cpu\n",
      "Total number of new clients: 0\n",
      "Fine tuning epoches on new clients: 0\n",
      "==================================================\n",
      "\n",
      "============= Running time: 0th =============\n",
      "Creating server and clients ...\n",
      "Digit5CNN(\n",
      "  (encoder): Sequential(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "  )\n",
      "  (linear): Sequential(\n",
      "    (fc1): Linear(in_features=8192, out_features=3072, bias=True)\n",
      "    (bn4): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU()\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (fc2): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "    (bn5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu5): ReLU()\n",
      "  )\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-331a451dc2cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fine tuning epoches on new clients: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfine_tuning_epoch_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-d5c53c951245>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# select algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"FedAvg\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFedAvg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-28599c289211>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, times)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# select slow clients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_slow_clients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclientAVG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nJoin ratio / total clients: {self.join_ratio} / {self.num_clients}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b1d36d27d66d>\u001b[0m in \u001b[0;36mset_clients\u001b[1;34m(self, clientObj)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_clients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclientObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_slow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msend_slow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_clients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_slow_clients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_slow_clients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_client_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_client_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             client = clientObj(self.args, \n",
      "\u001b[1;32m<ipython-input-4-ca22a5e5c30c>\u001b[0m in \u001b[0;36mread_client_data\u001b[1;34m(dataset, idx, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ca22a5e5c30c>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(dataset, idx, is_train)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 return format.read_array(bytes,\n\u001b[0m\u001b[0;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                                          pickle_kwargs=self.pickle_kwargs)\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;31m# Friendlier error message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[0;32m   1018\u001b[0m                          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "conf = args_dict = {\n",
    "    \"goal\": \"noniid\",# 有无差异\n",
    "    \"device\": \"cuda\",\n",
    "    \"device_id\": \"0\",\n",
    "    \"dataset\": \"Digit5\", # 数据集\n",
    "    \"num_classes\": 10,  \n",
    "    \"model\": \"cnn\",  # 模型\n",
    "    \"batch_size\": 10,  \n",
    "    \"local_learning_rate\": 0.005,\n",
    "    \"learning_rate_decay\": False,\n",
    "    \"learning_rate_decay_gamma\": 0.99,\n",
    "    \"global_rounds\": 50,  # 训练轮次\n",
    "    \"local_epochs\": 1,\n",
    "    \"algorithm\": \"FedAvg\",  # 算法 FedAvg/FedAMP\n",
    "    \"join_ratio\": 1.0,\n",
    "    \"random_join_ratio\": False,\n",
    "    \"num_clients\": 5,  # 客户端数量\n",
    "    \"prev\": 0,\n",
    "    \"times\": 1,\n",
    "    \"eval_gap\": 1,\n",
    "    \"save_folder_name\": \"items\",\n",
    "    \"batch_num_per_client\": 2,\n",
    "    \"num_new_clients\": 0,\n",
    "    \"fine_tuning_epoch_new\": 0,\n",
    "    \"client_drop_rate\": 0.0,\n",
    "    \"train_slow_rate\": 0.0,\n",
    "    \"send_slow_rate\": 0.0,\n",
    "    \"time_select\": False,\n",
    "    \"time_threthold\": 10000,\n",
    "    \"beta\": 0.0,\n",
    "    \"lamda\": 1.0,\n",
    "    \"mu\": 0.0,\n",
    "    \"K\": 5,\n",
    "    \"p_learning_rate\": 0.01,\n",
    "    \"alphaK\": 1.0,\n",
    "    \"sigma\": 1.0,\n",
    "}\n",
    "\n",
    "\n",
    "args = argparse.Namespace(**conf)\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device_id\n",
    "\n",
    "if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "    print(\"\\ncuda is not avaiable.\\n\")\n",
    "    args.device = \"cpu\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Algorithm: {}\".format(args.algorithm))\n",
    "print(\"Local batch size: {}\".format(args.batch_size))\n",
    "print(\"Local epochs: {}\".format(args.local_epochs))\n",
    "print(\"Local learing rate: {}\".format(args.local_learning_rate))\n",
    "print(\"Local learing rate decay: {}\".format(args.learning_rate_decay))\n",
    "if args.learning_rate_decay:\n",
    "    print(\"Local learing rate decay gamma: {}\".format(args.learning_rate_decay_gamma))\n",
    "print(\"Total number of clients: {}\".format(args.num_clients))\n",
    "print(\"Clients join in each round: {}\".format(args.join_ratio))\n",
    "print(\"Clients randomly join: {}\".format(args.random_join_ratio))\n",
    "print(\"Client drop rate: {}\".format(args.client_drop_rate))\n",
    "print(\"Client select regarding time: {}\".format(args.time_select))\n",
    "if args.time_select:\n",
    "    print(\"Time threthold: {}\".format(args.time_threthold))\n",
    "print(\"Running times: {}\".format(args.times))\n",
    "print(\"Dataset: {}\".format(args.dataset))\n",
    "print(\"Number of classes: {}\".format(args.num_classes))\n",
    "print(\"Backbone: {}\".format(args.model))\n",
    "print(\"Using device: {}\".format(args.device))\n",
    "\n",
    "if args.device == \"cuda\":\n",
    "    print(\"Cuda device id: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "print(\"Total number of new clients: {}\".format(args.num_new_clients))\n",
    "print(\"Fine tuning epoches on new clients: {}\".format(args.fine_tuning_epoch_new))\n",
    "print(\"=\" * 50)\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_exp_data(filename):\n",
    "    file_path = \"./results/\" + filename\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        rs_test_acc = np.array(hf.get('rs_test_acc'))\n",
    "    return rs_test_acc\n",
    "\n",
    "\n",
    "def plot_multi_compare(data_dict, save_fig=None, title=\"Acc Comparsion\"):\n",
    "    plt.figure()\n",
    "    for k, v in data_dict.items():\n",
    "        plt.plot(range(len(v)), v, label=k)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Testing Accuracy')\n",
    "    plt.xlabel(\"global epoch\")\n",
    "    plt.title(title)\n",
    "    if save_fig:\n",
    "        plt.savefig(os.path.join(save_fig))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"FedAvg\": get_exp_data(\"Digit5_FedAvg_iid_0.h5\")\n",
    "}\n",
    "plot_multi_compare(data_dict, title=\"Digit5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"FedAMP\": get_exp_data(\"Digit5_FedAMP_iid_0\"),\n",
    "    \"FedAvg\": get_exp_data(\"Digit5_FedAvg_iid_0\")\n",
    "}\n",
    "\n",
    "plot_multi_compare(data_dict, title=\"Digit5 on iid\")\n",
    "\n",
    "data_dict = {\n",
    "    \"FedAvg-iid\": get_exp_data(\"Digit5_FedAvg_iid_0\"),\n",
    "    \"FedAMP-noniid\": get_exp_data(\"Digit5_FedAmp_noniid_0\"),\n",
    "    \"FedAvg-noniid\": get_exp_data(\"Digit5_FedAvg_noniid_0\")\n",
    "}\n",
    "\n",
    "plot_multi_compare(data_dict, title=\"Digit5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
